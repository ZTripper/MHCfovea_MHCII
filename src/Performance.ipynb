{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, copy, re, random\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statannot import add_stat_annotation\n",
    "import sklearn.metrics as metrics\n",
    "from util import *\n",
    "from scipy.stats.mstats import ttest_rel, ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsampling Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownsamplingPerformance():\n",
    "    def __init__(self, prediction_file, decoy_fold_per_model, output_dir, index=None):\n",
    "        self.df = pd.read_csv(prediction_file, index_col=0)\n",
    "        self.decoy_fold_per_model = decoy_fold_per_model\n",
    "        self.output_dir = output_dir\n",
    "        if not os.path.isdir(self.output_dir):\n",
    "            os.mkdir(self.output_dir)\n",
    "        \n",
    "        if index:\n",
    "            self.df = self.df.loc[index].reset_index(drop=True)\n",
    "\n",
    "        # metrics_dict\n",
    "        bind = self.df['bind']\n",
    "        decoy_list = sorted([int(i.split('_')[1]) for i in self.df.columns if 'decoy' in i])\n",
    "        self.metrics_dict = OrderedDict()\n",
    "        for i in tqdm(range(len(decoy_list))):\n",
    "            cols = ['decoy_%d'%i for i in decoy_list[:i+1]]\n",
    "            pred = self.df[cols].mean(axis=1)\n",
    "            self.metrics_dict[decoy_list[i]] = CalculateMetrics(bind, pred)\n",
    "    \n",
    "    \n",
    "    def __call__(self):\n",
    "        # save metrics_dict\n",
    "        json.dump(self.metrics_dict, open('%s/MetricsDict.json'%self.output_dir, 'w'))\n",
    "        \n",
    "        # plot metrics\n",
    "        metrics_list = ['AUC', 'AUC0.1', 'AP', 'PPV']\n",
    "        for m in metrics_list:\n",
    "            self._plot_metrics(m)\n",
    "    \n",
    "    \n",
    "    def _plot_metrics(self, metrics_name, savefig=True, figsize=(3.5,3.5), dpi=600, fontsize=10, linewidth=1.5):\n",
    "        metrics_label = {'AUC': 'AUC-ROC', 'AP': 'AUC-PRC', 'AUC0.1': 'AUC-ROC 0.1', 'PPV': 'PPV'}\n",
    "        fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "        x = list(self.metrics_dict.keys())\n",
    "        y = [self.metrics_dict[i][metrics_name] for i in x]\n",
    "        sns.lineplot(x=x, y=y, linewidth=linewidth)\n",
    "        plt.title(metrics_label[metrics_name], fontsize=fontsize)\n",
    "        plt.xlabel('score', fontsize=fontsize)\n",
    "        plt.ylabel('decoy number', fontsize=fontsize)\n",
    "        plt.xticks(fontsize=fontsize)\n",
    "        plt.yticks(fontsize=fontsize)\n",
    "        fig.tight_layout()\n",
    "        if savefig:\n",
    "            fig.savefig('%s/Metrics_%s.png'%(self.output_dir, metrics_name))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "\n",
    "select_decoy = True\n",
    "downsampling_factors = [1,5,10,15,30]\n",
    "downsampling_outdir = '../analysis/performance/downsampling'\n",
    "if not os.path.isdir(downsampling_outdir):\n",
    "    os.mkdir(downsampling_outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid dataframe index for 30n decoys\n",
    "\n",
    "if select_decoy:\n",
    "    valid_df = pd.read_csv('../data/raw/dataframe/valid.csv', index_col=0)\n",
    "    index = list(valid_df[valid_df['source']=='MS'].index)\n",
    "    num = len(index)*15\n",
    "    index += list(valid_df[valid_df['source']=='assay'].index)\n",
    "    index += list(valid_df[valid_df['source']=='protein_decoy'].index)\n",
    "    index += list(valid_df[valid_df['source'].str.contains('random_decoy')].sample(n=num, random_state=0).index)\n",
    "else:\n",
    "    index = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Downsampling results\n",
    "for factor in downsampling_factors:\n",
    "    model_num = 93//factor\n",
    "    prediction_file = '../prediction/valid/res182_decoy%d_CNN_1_1_%d/tmp_prediction.csv'%(factor, model_num)\n",
    "    output_dir = '%s/res182_decoy%d_CNN_1_1'%(downsampling_outdir, factor)\n",
    "    DP = DownsamplingPerformance(prediction_file, factor, output_dir, index)\n",
    "    DP()\n",
    "    print(\"Factor %d Complete\"%factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison between models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "\n",
    "downsampling_factors = [1,5,10,15,30]\n",
    "comparison_decoy_num_list = [30, 60, 90]\n",
    "downsampling_outdir = '../analysis/performance/downsampling'\n",
    "\n",
    "metrics_list = ['AP', 'AUC']\n",
    "\n",
    "figsize = (4.5,3.5)\n",
    "dpi = 600\n",
    "fontsize = 10\n",
    "linewidth = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=figsize, dpi=dpi)\n",
    "current_ax = 0\n",
    "\n",
    "for metrics_name in metrics_list:\n",
    "    # Comparison\n",
    "    metrics_dict = dict()\n",
    "    for factor in downsampling_factors:\n",
    "        file = '%s/res182_decoy%d_CNN_1_1/MetricsDict.json'%(downsampling_outdir, factor)\n",
    "        metrics_dict[factor] = json.load(open(file, 'r'))\n",
    "\n",
    "    comparison_list = list()\n",
    "    for decoy_num in comparison_decoy_num_list:\n",
    "        for factor in downsampling_factors:\n",
    "            comparison_list.append({\n",
    "                'downsampling_factor': 'factor_%d'%factor,\n",
    "                'decoy_num': decoy_num,\n",
    "                metrics_name: metrics_dict[factor][str(decoy_num-factor+1)][metrics_name]\n",
    "            })\n",
    "    comparison_df = pd.DataFrame(comparison_list)\n",
    "    \n",
    "    # Plot\n",
    "    sns.lineplot(data=comparison_df, x='decoy_num', y=metrics_name, hue='downsampling_factor',\n",
    "                 linewidth=linewidth, ax=ax[current_ax])\n",
    "    ax[current_ax].set_title(metrics_name, fontsize=fontsize)\n",
    "    ax[current_ax].set_xlabel(None)\n",
    "    ax[current_ax].set_ylabel(None)\n",
    "    ax[current_ax].set_xticks([i for i in comparison_decoy_num_list])\n",
    "    ax[current_ax].set_xticklabels(['%dn'%i for i in comparison_decoy_num_list], fontsize=fontsize)\n",
    "    for i in ax[current_ax].get_yticklabels():\n",
    "        i.set_fontsize(fontsize)\n",
    "    \n",
    "    h, l = ax[current_ax].get_legend_handles_labels()\n",
    "    ax[current_ax].get_legend().remove()\n",
    "    \n",
    "    current_ax += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "l = [\"Downsampling Factor\"] + downsampling_factors\n",
    "leg = fig.legend(h, l, fontsize=fontsize,\n",
    "                 ncol=6, loc=\"upper center\", bbox_to_anchor=(0, 1, 1, 0.1), \n",
    "                 columnspacing=1, handlelength=0.5, handletextpad=0.2, borderpad=0.2)\n",
    "\n",
    "fig.add_subplot(111, frame_on=False)\n",
    "plt.tick_params(labelcolor=\"none\", bottom=False, left=False)\n",
    "plt.xlabel(\"Decoy Number\", fontsize=fontsize)\n",
    "\n",
    "fig.savefig('%s/DownsamplingComparison.png'%downsampling_outdir, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df\n",
    "\n",
    "test_file = '../prediction/test/prediction.csv'\n",
    "test_df = pd.read_csv(test_file, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments\n",
    "\n",
    "with_mixmhcpred = False\n",
    "previous_convert_dict = False\n",
    "\n",
    "if with_mixmhcpred:\n",
    "    tool_list = ['MHCfovea', 'NetMHCpan4.1', 'MHCflurry2.0', 'MixMHCpred2.1']\n",
    "    test_df = test_df[~test_df['MixMHCpred2.1'].isna()]\n",
    "else:\n",
    "    tool_list = ['MHCfovea', 'NetMHCpan4.1', 'MHCflurry2.0']\n",
    "\n",
    "if with_mixmhcpred:\n",
    "    output_dir = '../analysis/performance/tool_comparison/with_mixmhcpred/'\n",
    "else:\n",
    "    output_dir = '../analysis/performance/tool_comparison/without_mixmhcpred/'\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot AUC-ROC comparing with other tools\n",
    "\n",
    "figsize = (3.5,3.5)\n",
    "dpi = 600\n",
    "fontsize = 10\n",
    "linewidth = 1.5\n",
    "\n",
    "fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "plt.title('ROC', fontsize=fontsize)\n",
    "\n",
    "for col in tool_list:\n",
    "    pred = test_df[col]\n",
    "    fpr, tpr, _ = metrics.roc_curve(test_df['bind'], pred)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label='%s,AUC=%.3f'%(col, auc), linewidth=linewidth)\n",
    "\n",
    "plt.legend(loc = 'lower right', fontsize=fontsize, handletextpad=0.2, borderpad=0.2)\n",
    "plt.plot([0, 1], [0, 1], '--', color='black')\n",
    "#plt.plot([0.1, 0.1], [0, 1], '--', color='red')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate', fontsize=fontsize)\n",
    "plt.xlabel('False Positive Rate', fontsize=fontsize)\n",
    "plt.xticks(fontsize=fontsize)\n",
    "plt.yticks(fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('%s/ComparisonROC.png'%output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot AUC-PRC comparing with netMHCpan4.1 and MHCflurry2.0\n",
    "\n",
    "figsize = (3.5,3.5)\n",
    "dpi = 600\n",
    "fontsize = 10\n",
    "linewidth = 1.5\n",
    "\n",
    "fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "plt.title('PRC', fontsize=fontsize)\n",
    "\n",
    "for col in tool_list:\n",
    "    pred = test_df[col]\n",
    "    precision, recall, _ = metrics.precision_recall_curve(test_df['bind'], pred)\n",
    "    auc = metrics.average_precision_score(test_df['bind'], pred)\n",
    "    plt.plot(precision, recall, label='%s,AP=%.3f'%(col, auc), linewidth=linewidth)\n",
    "\n",
    "plt.legend(loc = 'lower left', fontsize=fontsize, handletextpad=0.2, borderpad=0.2)\n",
    "#plt.plot([0, 1], [0, 1],'--',color='black')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('Recall', fontsize=fontsize)\n",
    "plt.xlabel('Precision', fontsize=fontsize)\n",
    "plt.xticks(fontsize=fontsize)\n",
    "plt.yticks(fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('%s/ComparisonPRC.png'%output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get allele metrics\n",
    "\n",
    "if previous_convert_dict:\n",
    "    convert_dict = json.load(open('%s/AlleleMetrics.json'%output_dir, 'r'))\n",
    "\n",
    "else:\n",
    "    allele_metrics_dict = dict()\n",
    "    for col in tool_list:\n",
    "        allele_metrics_dict[col] = CalculateAlleleMetrics(test_df['mhc'], test_df['bind'], test_df[col])\n",
    "\n",
    "    convert_dict = dict({'AUC':list(), 'AUC0.1':list(), 'AP':list(), 'PPV':list()})\n",
    "    for method, allele_metrics in allele_metrics_dict.items():\n",
    "        for allele, metrics_dict in allele_metrics.items():\n",
    "            for metric, val in metrics_dict.items():\n",
    "                convert_dict[metric].append({'allele': allele, 'value': val, 'method': method})\n",
    "\n",
    "    json.dump(convert_dict, open('%s/AlleleMetrics.json'%output_dir, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot metrics by allele\n",
    "\n",
    "figsize = (3.5,3.5)\n",
    "dpi = 600\n",
    "fontsize = 10\n",
    "linewidth = 1.5\n",
    "\n",
    "metric_list = ['AUC', 'AUC0.1', 'AP', 'PPV']\n",
    "box_pairs = list()\n",
    "\n",
    "for i in range(1, len(tool_list)):\n",
    "    box_pairs.append((tool_list[0], tool_list[i]))\n",
    "\n",
    "for i in range(len(metric_list)):\n",
    "    metric = metric_list[i]\n",
    "    temp_df = pd.DataFrame(convert_dict[metric])\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize, dpi=dpi)\n",
    "    sns.stripplot(x='method', y='value', data=temp_df, ax=ax, s=2)\n",
    "    sns.violinplot(x='method', y='value', data=temp_df, ax=ax, color=\".8\")\n",
    "    \n",
    "    ax.set_title(metric)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_yticks([i for i in ax.get_yticks() if i <= 1.0])\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(fontsize)\n",
    "    for item in ax.get_xticklabels():\n",
    "        item.set_rotation(45)\n",
    "    \n",
    "    test_results = add_stat_annotation(ax=ax, data=temp_df, x='method', y='value',\n",
    "                                       box_pairs=box_pairs, test='t-test_ind', comparisons_correction=None,\n",
    "                                       text_format='star', loc='inside',\n",
    "                                       fontsize=fontsize, linewidth=linewidth,\n",
    "                                       line_offset_to_box=0.15)\n",
    "    \n",
    "    fig.savefig('%s/AlleleGroup%s.png'%(output_dir, metric), bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unobserved Alleles\n",
    "set(alleles of testing dataset) - set(alleles of training hit dataset, # > 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = '../analysis/performance/tool_comparison/without_mixmhcpred/'\n",
    "convert_dict = json.load(open('%s/AlleleMetrics.json'%output_dir, 'r'))\n",
    "\n",
    "figsize = (3.5,3.5)\n",
    "dpi = 600\n",
    "fontsize = 10\n",
    "linewidth = 1.5\n",
    "\n",
    "metric_list = ['AUC', 'AUC0.1', 'AP', 'PPV']\n",
    "\n",
    "rare_alleles = ['A*02:05', 'A*11:02', 'A*24:07', 'A*33:03', 'A*34:01',\n",
    "                'A*34:02', 'A*36:01', 'A*74:01', 'B*07:04', 'B*13:01',\n",
    "                'B*13:02', 'B*15:10', 'B*35:07', 'B*37:01', 'B*38:02',\n",
    "                'B*40:06', 'B*52:01', 'B*55:01', 'B*55:02', 'B*58:02',\n",
    "                'C*03:02', 'C*04:03', 'C*07:04', 'C*08:01', 'C*14:03']\n",
    "\n",
    "consensus_rare_alleles = ['A*24:07', 'A*34:01', 'A*34:02', 'A*36:01',\n",
    "                          'B*07:04', 'B*35:07', 'B*38:02', 'B*40:06',\n",
    "                          'C*03:02', 'C*04:03', 'C*14:03']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# unobserved alleles v.s. observed alleles\n",
    "\n",
    "box_pairs=[('Observed Allele', 'Unobserved Allele')]\n",
    "\n",
    "for i in range(len(metric_list)):\n",
    "    metric = metric_list[i]\n",
    "    temp_df = pd.DataFrame(convert_dict[metric])\n",
    "    temp_df = temp_df[temp_df['method']=='MHCfovea']\n",
    "    temp_df['tag'] = 'Observed Allele'\n",
    "    temp_df.loc[temp_df['allele'].isin(rare_alleles), 'tag'] = 'Unobserved Allele'\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize, dpi=dpi)\n",
    "    sns.stripplot(x='tag', y='value', data=temp_df, ax=ax, s=2)\n",
    "    sns.violinplot(x='tag', y='value', data=temp_df, ax=ax, color=\".8\")\n",
    "    \n",
    "    ax.set_title(metric)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_yticks([i for i in ax.get_yticks() if i <= 1.0])\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(fontsize)\n",
    "    \n",
    "    test_results = add_stat_annotation(ax=ax, data=temp_df, x='tag', y='value',\n",
    "                                       box_pairs=box_pairs, test='t-test_ind', comparisons_correction=None,\n",
    "                                       text_format='star', loc='inside',\n",
    "                                       fontsize=fontsize, linewidth=linewidth,\n",
    "                                       line_offset_to_box=0.15)\n",
    "    \n",
    "    fig.savefig('../analysis/performance/rare_allele/RareAllele%s.png'%metric, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# comparison with other tools on unobserved alleles\n",
    "\n",
    "tool_list = ['MHCfovea', 'NetMHCpan4.1', 'MHCflurry2.0']\n",
    "box_pairs = list()\n",
    "\n",
    "for i in range(1, len(tool_list)):\n",
    "    box_pairs.append((tool_list[0], tool_list[i]))\n",
    "\n",
    "for i in range(len(metric_list)):\n",
    "    metric = metric_list[i]\n",
    "    temp_df = pd.DataFrame(convert_dict[metric])\n",
    "    temp_df = temp_df[temp_df['allele'].isin(consensus_rare_alleles)]\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize, dpi=dpi)\n",
    "    sns.stripplot(x='method', y='value', data=temp_df, ax=ax, s=2)\n",
    "    sns.violinplot(x='method', y='value', data=temp_df, ax=ax, color=\".8\")\n",
    "    \n",
    "    ax.set_title(metric)\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_yticks([i for i in ax.get_yticks() if i <= 1.0])\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(fontsize)\n",
    "    for item in ax.get_xticklabels():\n",
    "        item.set_rotation(45)\n",
    "    \n",
    "    test_results = add_stat_annotation(ax=ax, data=temp_df, x='method', y='value',\n",
    "                                       box_pairs=box_pairs, test='t-test_ind', comparisons_correction=None,\n",
    "                                       text_format='star', loc='inside',\n",
    "                                       fontsize=fontsize, linewidth=linewidth,\n",
    "                                       line_offset_to_box=0.15)\n",
    "    \n",
    "    fig.savefig('../analysis/performance/rare_allele/RareAlleleGroup%s.png'%metric, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
