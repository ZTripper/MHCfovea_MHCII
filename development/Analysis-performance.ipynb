{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, copy, re, random\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statannot import add_stat_annotation\n",
    "import sklearn.metrics as metrics\n",
    "from scipy.stats import ttest_ind, t\n",
    "from util import *\n",
    "from IPython.display import display\n",
    "\n",
    "valid_file = ''\n",
    "benchmark_file = ''\n",
    "work_dir = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid dataframe\n",
    "valid_df = pd.read_csv(valid_file, index_col=0)\n",
    "\n",
    "## index (D-E ratio about 30)\n",
    "index = list(valid_df[valid_df['source']=='MS'].index)\n",
    "num = len(index)*15\n",
    "index += list(valid_df[valid_df['source']=='assay'].index)\n",
    "index += list(valid_df[valid_df['source']=='protein_decoy'].index)\n",
    "index += list(valid_df[valid_df['source'].str.contains('random_decoy')].sample(n=num, random_state=0).index)\n",
    "\n",
    "valid_df = valid_df.iloc[index]\n",
    "bind = valid_df['bind'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns\n",
    "cols = [i for i in valid_df.columns if 'batch_size' in i]\n",
    "\n",
    "# performance\n",
    "perform_list = list()\n",
    "for col in cols:\n",
    "    batch_size = col.split('_')[2]\n",
    "    learning_rate = col.split('_')[-1]\n",
    "    pred = valid_df[col].to_numpy()\n",
    "    metric = CalculateMetrics(bind, pred)\n",
    "    perform_list.append({\n",
    "        'batch_size': batch_size,\n",
    "        'learning_rate': learning_rate,\n",
    "        'AUC': metric['AUC'],\n",
    "        'AUC0.1': metric['AUC0.1'],\n",
    "        'AP': metric['AP'],\n",
    "        'PPV': metric['PPV']\n",
    "    })\n",
    "perform_df = pd.DataFrame(perform_list)\n",
    "display(perform_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downsampling Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns\n",
    "cols = [i for i in valid_df.columns if 'DE' in i]\n",
    "\n",
    "# performance\n",
    "perform_list = list()\n",
    "for col in cols:\n",
    "    DE_downsized = col.split('_')[1]\n",
    "    DE_training = col.split('_')[-1]\n",
    "    pred = valid_df[col].to_numpy()\n",
    "    metric = CalculateMetrics(bind, pred)\n",
    "    perform_list.append({\n",
    "        'DE_downsized': DE_downsized,\n",
    "        'DE_training': DE_training,\n",
    "        'AUC': metric['AUC'],\n",
    "        'AUC0.1': metric['AUC0.1'],\n",
    "        'AP': metric['AP'],\n",
    "        'PPV': metric['PPV']\n",
    "    })\n",
    "perform_df = pd.DataFrame(perform_list)\n",
    "display(perform_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "DE_downsized_list = [1,5,10,15,30]\n",
    "DE_training_list = [30, 60, 90]\n",
    "\n",
    "metrics_list = ['AP', 'AUC']\n",
    "\n",
    "figsize = (4.5,3.5)\n",
    "dpi = 600\n",
    "fontsize = 10\n",
    "linewidth = 1.5\n",
    "\n",
    "# figure\n",
    "fig, ax = plt.subplots(1, 2, figsize=figsize, dpi=dpi)\n",
    "current_ax = 0\n",
    "\n",
    "for metrics_name in metrics_list:\n",
    "    # Plot\n",
    "    sns.lineplot(data=perform_df, x='DE_training', y=metrics_name, hue='DE_downsized',\n",
    "                 linewidth=linewidth, ax=ax[current_ax])\n",
    "    ax[current_ax].set_title(metrics_name, fontsize=fontsize)\n",
    "    ax[current_ax].set_xlabel(None)\n",
    "    ax[current_ax].set_ylabel(None)\n",
    "    ax[current_ax].set_xticks([i for i in range(len(DE_training_list))])\n",
    "    ax[current_ax].set_xticklabels(['%d'%i for i in DE_training_list], fontsize=fontsize)\n",
    "    for i in ax[current_ax].get_yticklabels():\n",
    "        i.set_fontsize(fontsize)\n",
    "    \n",
    "    h, l = ax[current_ax].get_legend_handles_labels()\n",
    "    ax[current_ax].get_legend().remove()\n",
    "    \n",
    "    current_ax += 1\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "l = DE_downsized_list\n",
    "leg = fig.legend(h, l, fontsize=fontsize, title=\"D-E ratio in each downsized dataset\",\n",
    "                 ncol=5, loc=\"upper center\", bbox_to_anchor=(0.05, 1.05, 1, 0.1), \n",
    "                 columnspacing=1, handlelength=0.5, handletextpad=0.2, borderpad=0.2)\n",
    "\n",
    "fig.add_subplot(111, frame_on=False)\n",
    "plt.tick_params(labelcolor=\"none\", bottom=False, left=False)\n",
    "plt.xlabel(\"D-E ratio in the training dataset\", fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WelchTest(x1, x2):\n",
    "    np.set_printoptions(precision=2)\n",
    "    n1 = x1.size\n",
    "    n2 = x2.size\n",
    "    m1 = np.mean(x1)\n",
    "    m2 = np.mean(x2)\n",
    "    v1 = np.var(x1, ddof=1)\n",
    "    v2 = np.var(x2, ddof=1)\n",
    "    \n",
    "    pooled_se = np.sqrt(v1 / n1 + v2 / n2)\n",
    "    delta = m1-m2\n",
    "    \n",
    "    tstat = delta / pooled_se\n",
    "    df = (v1 / n1 + v2 / n2)**2 / (v1**2 / (n1**2 * (n1 - 1)) + v2**2 / (n2**2 * (n2 - 1)))\n",
    "    \n",
    "    # two side t-test\n",
    "    p = 2 * t.cdf(-abs(tstat), df)\n",
    "    \n",
    "    # upper and lower bounds\n",
    "    lb = delta - t.ppf(0.975,df)*pooled_se \n",
    "    ub = delta + t.ppf(0.975,df)*pooled_se\n",
    "    \n",
    "    # stat dict\n",
    "    stat_dict = {\n",
    "        'n': [n1,n2],\n",
    "        'm': [m1,m2],\n",
    "        'sd': [np.sqrt(v1), np.sqrt(v2)],\n",
    "        'df': df,\n",
    "        'psd': pooled_se,\n",
    "        'tstat': tstat,\n",
    "        'delta': delta,\n",
    "        'pvalue': p,\n",
    "        'lb': lb,\n",
    "        'ub': ub\n",
    "    }\n",
    "  \n",
    "    return stat_dict\n",
    "\n",
    "\n",
    "def PrintStatDF(df):\n",
    "    temp_df = pd.DataFrame(index=['n','m','sd','df','psd','tstat','pvalue','lb','ub'])\n",
    "    for idx, row in df.iterrows():\n",
    "        pair_1, pair_2 = row['pair'].split('-')\n",
    "\n",
    "        temp_df['{}_{}_{}'.format(pair_1, idx, row['metric'])] = [\n",
    "            row['n'][0],\n",
    "            row['m'][0],\n",
    "            row['sd'][0],\n",
    "            row['df'],\n",
    "            row['psd'],\n",
    "            row['tstat'],\n",
    "            row['pvalue'],\n",
    "            row['lb'],\n",
    "            row['ub']\n",
    "        ]\n",
    "\n",
    "        temp_df['{}_{}_{}'.format(pair_2, idx, row['metric'])] = [\n",
    "            row['n'][1],\n",
    "            row['m'][1],\n",
    "            row['sd'][1],\n",
    "            row['df'],\n",
    "            row['psd'],\n",
    "            row['tstat'],\n",
    "            row['pvalue'],\n",
    "            row['lb'],\n",
    "            row['ub']\n",
    "        ]\n",
    "    return temp_df\n",
    "\n",
    "\n",
    "def ViolinPlotStat(df, xcol, ycol, box_pairs, figfile,\n",
    "                   title=None, xlabel=None, ylabel=None, ytick_limit=1.0, xtick_rotate=45,\n",
    "                   figsize=(3.5, 3.5), dpi=600, fontsize=10, linewidth=1.5):\n",
    "    \n",
    "    # plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=figsize, dpi=dpi)\n",
    "    sns.stripplot(x=xcol, y=ycol, data=df, ax=ax, s=2)\n",
    "    sns.violinplot(x=xcol, y=ycol, data=df, ax=ax, color='.8', cut=0)\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_yticks([i for i in ax.get_yticks() if i <= ytick_limit])\n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(fontsize)\n",
    "    for item in ax.get_xticklabels():\n",
    "        item.set_rotation(xtick_rotate)\n",
    "        \n",
    "    # stats\n",
    "    test_results = add_stat_annotation(ax=ax, data=df, x=xcol, y=ycol,\n",
    "                                       box_pairs=box_pairs, comparisons_correction=None,\n",
    "                                       test='t-test_ind', stats_params={'equal_var': False},\n",
    "                                       text_format='star', loc='inside',\n",
    "                                       fontsize=fontsize, linewidth=linewidth,\n",
    "                                       line_offset_to_box=0.15)\n",
    "    \n",
    "    stat_dict_list = list()\n",
    "    for p1, p2 in box_pairs:\n",
    "        stat_dict = WelchTest(df[df[xcol]==p1][ycol], df[df[xcol]==p2][ycol])\n",
    "        stat_dict['pair'] = '{}-{}'.format(p1, p2)\n",
    "        stat_dict_list.append(stat_dict)\n",
    "        \n",
    "    # savefig\n",
    "    fig.savefig(figfile, bbox_inches='tight')\n",
    "    \n",
    "    return stat_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df\n",
    "\n",
    "test_df = pd.read_csv(test_file, index_col=0)\n",
    "\n",
    "# arguments\n",
    "with_mixmhcpred = True\n",
    "previous_convert_dict = True\n",
    "\n",
    "# with/without MixMHCpred2.1\n",
    "if with_mixmhcpred:\n",
    "    tool_list = ['MHCfovea', 'NetMHCpan4.1', 'MHCflurry2.0', 'MixMHCpred2.1']\n",
    "    test_df = test_df[~test_df['MixMHCpred2.1'].isna()]\n",
    "else:\n",
    "    tool_list = ['MHCfovea', 'NetMHCpan4.1', 'MHCflurry2.0']\n",
    "\n",
    "if with_mixmhcpred:\n",
    "    output_dir = '%s/with_mixmhcpred/'%work_dir\n",
    "else:\n",
    "    output_dir = '%s/without_mixmhcpred/'%work_dir\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "    \n",
    "# get allele metrics\n",
    "if previous_convert_dict:\n",
    "    convert_dict = json.load(open('%s/AlleleMetrics.json'%output_dir, 'r'))\n",
    "\n",
    "else:\n",
    "    allele_metrics_dict = dict()\n",
    "    for col in tool_list:\n",
    "        allele_metrics_dict[col] = CalculateAlleleMetrics(test_df['mhc'], test_df['bind'], test_df[col])\n",
    "\n",
    "    convert_dict = dict({'AUC':list(), 'AUC0.1':list(), 'AP':list(), 'PPV':list()})\n",
    "    for method, allele_metrics in allele_metrics_dict.items():\n",
    "        for allele, metrics_dict in allele_metrics.items():\n",
    "            for metric, val in metrics_dict.items():\n",
    "                convert_dict[metric].append({'allele': allele, 'value': val, 'method': method})\n",
    "\n",
    "    json.dump(convert_dict, open('%s/AlleleMetrics.json'%output_dir, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### overall benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metrics\n",
    "\n",
    "metrics_dict = dict()\n",
    "y = test_df['bind'].to_numpy()\n",
    "for tool in tool_list:\n",
    "    metrics_dict[tool] = CalculateMetrics(y, test_df[tool].to_numpy())\n",
    "\n",
    "pd.DataFrame(metrics_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot AUC-ROC comparing with other tools\n",
    "\n",
    "figsize = (3.5,3.5)\n",
    "dpi = 600\n",
    "fontsize = 10\n",
    "linewidth = 1.5\n",
    "\n",
    "fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "plt.title('ROC', fontsize=fontsize)\n",
    "\n",
    "for col in tool_list:\n",
    "    pred = test_df[col]\n",
    "    fpr, tpr, _ = metrics.roc_curve(test_df['bind'], pred)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label='%s,AUC=%.3f'%(col, auc), linewidth=linewidth)\n",
    "\n",
    "plt.legend(loc = 'lower right', fontsize=fontsize, handletextpad=0.2, borderpad=0.2)\n",
    "plt.plot([0, 1], [0, 1], '--', color='black')\n",
    "#plt.plot([0.1, 0.1], [0, 1], '--', color='red')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate', fontsize=fontsize)\n",
    "plt.xlabel('False Positive Rate', fontsize=fontsize)\n",
    "plt.xticks(fontsize=fontsize)\n",
    "plt.yticks(fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('%s/ComparisonROC.png'%output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot AUC-PRC comparing with other tools\n",
    "\n",
    "figsize = (3.5,3.5)\n",
    "dpi = 600\n",
    "fontsize = 10\n",
    "linewidth = 1.5\n",
    "\n",
    "fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "plt.title('PRC', fontsize=fontsize)\n",
    "\n",
    "for col in tool_list:\n",
    "    pred = test_df[col]\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(test_df['bind'], pred)\n",
    "    auc = metrics.average_precision_score(test_df['bind'], pred)\n",
    "    plt.plot(precision, recall, label='%s,AP=%.3f'%(col, auc), linewidth=linewidth)\n",
    "    \n",
    "    # search max f1_score\n",
    "    f1_scores = 2*recall*precision/(recall+precision)\n",
    "    print('Tool: ', col)\n",
    "    print('Best threshold: ', thresholds[np.argmax(f1_scores)])\n",
    "    print('Best F1-Score: ', np.max(f1_scores))\n",
    "\n",
    "plt.legend(loc = 'lower left', fontsize=fontsize, handletextpad=0.2, borderpad=0.2)\n",
    "#plt.plot([0, 1], [0, 1],'--',color='black')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('Recall', fontsize=fontsize)\n",
    "plt.xlabel('Precision', fontsize=fontsize)\n",
    "plt.xticks(fontsize=fontsize)\n",
    "plt.yticks(fontsize=fontsize)\n",
    "plt.show()\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('%s/ComparisonPRC.png'%output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### by alleles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot metrics by allele\n",
    "\n",
    "stat_df = pd.DataFrame()\n",
    "\n",
    "# box pairs\n",
    "box_pairs = list()\n",
    "for i in range(1, len(tool_list)):\n",
    "    box_pairs.append((tool_list[0], tool_list[i]))\n",
    "\n",
    "# by metrics\n",
    "metric_list = ['AUC', 'AUC0.1', 'AP', 'PPV']\n",
    "for i in range(len(metric_list)):\n",
    "    metric = metric_list[i]\n",
    "    temp_df = pd.DataFrame(convert_dict[metric])\n",
    "    stat_list = ViolinPlotStat(temp_df, 'method', 'value', box_pairs,\n",
    "                               '{}/AlleleGroup{}.png'.format(output_dir, metric), ylabel=metric)\n",
    "    plt.show()\n",
    "    temp_df = pd.DataFrame(stat_list)\n",
    "    temp_df['metric'] = metric\n",
    "    stat_df = pd.concat([stat_df, temp_df])\n",
    "\n",
    "stat_df = PrintStatDF(stat_df)\n",
    "display(stat_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unobserved Alleles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_dict_file = '{}/without_mixmhcpred/AlleleMetrics.json'.format(work_dir)\n",
    "convert_dict = json.load(open(convert_dict_file, 'r'))\n",
    "\n",
    "output_dir = '{}/unobserved'.format(work_dir)\n",
    "\n",
    "metric_list = ['AUC', 'AUC0.1', 'AP', 'PPV']\n",
    "\n",
    "unobserved_alleles = ['A*24:07', 'A*33:03', 'A*34:01', 'A*34:02', 'A*36:01', 'B*07:04', 'B*15:10', 'B*35:07',\n",
    "                      'B*38:02', 'B*40:06', 'B*55:01', 'B*55:02', 'C*03:02', 'C*04:03', 'C*08:01', 'C*14:03']\n",
    "\n",
    "# MixMHCpred2.1 misses B*35:07\n",
    "consensus_rare_alleles = ['A*24:07', 'A*34:01', 'A*34:02', 'A*36:01',\n",
    "                          'B*07:04', 'B*35:07', 'B*38:02', 'B*40:06',\n",
    "                          'C*03:02', 'C*04:03', 'C*14:03']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# observed vs. unobserved alleles\n",
    "\n",
    "metrics_df = pd.DataFrame()\n",
    "stat_df = pd.DataFrame()\n",
    "\n",
    "# box pairs\n",
    "box_pairs = [('Observed Allele', 'Unobserved Allele')]\n",
    "\n",
    "# by metrics\n",
    "metric_list = ['AUC', 'AUC0.1', 'AP', 'PPV']\n",
    "for i in range(len(metric_list)):\n",
    "    metric = metric_list[i]\n",
    "    temp_df = pd.DataFrame(convert_dict[metric])\n",
    "    temp_df = temp_df[temp_df['method']=='MHCfovea']\n",
    "    temp_df = temp_df.sort_values(by='allele')\n",
    "    \n",
    "    if metrics_df.shape[0] == 0:\n",
    "        metrics_df = temp_df\n",
    "        metrics_df = metrics_df.rename(columns={'value': metric})\n",
    "        metrics_df['tag'] = 'Observed Allele'\n",
    "        metrics_df.loc[metrics_df['allele'].isin(unobserved_alleles), 'tag'] = 'Unobserved Allele'\n",
    "    else:\n",
    "        metrics_df[metric] = temp_df['value']\n",
    "    \n",
    "    stat_list = ViolinPlotStat(metrics_df, 'tag', metric, box_pairs,\n",
    "                               '{}/UnobservedAllele{}.png'.format(output_dir, metric), ylabel=metric, xtick_rotate=0)\n",
    "    plt.show()\n",
    "    temp_df = pd.DataFrame(stat_list)\n",
    "    temp_df['metric'] = metric\n",
    "    stat_df = pd.concat([stat_df, temp_df])\n",
    "\n",
    "metrics_df.to_csv('{}/UnobservedAlleleMetrics.csv'.format(output_dir))\n",
    "stat_df = PrintStatDF(stat_df)\n",
    "display(stat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# comparison with other tools on common unobserved alleles\n",
    "\n",
    "convert_dict_file = '{}/with_mixmhcpred/AlleleMetrics.json'.format(work_dir)\n",
    "convert_dict = json.load(open(convert_dict_file, 'r'))\n",
    "\n",
    "stat_df = pd.DataFrame()\n",
    "\n",
    "# box pairs\n",
    "tool_list = ['MHCfovea', 'NetMHCpan4.1', 'MHCflurry2.0', 'MixMHCpred2.1']\n",
    "box_pairs = list()\n",
    "for i in range(1, len(tool_list)):\n",
    "    box_pairs.append((tool_list[0], tool_list[i]))\n",
    "\n",
    "# by metrics\n",
    "metric_list = ['AUC', 'AUC0.1', 'AP', 'PPV']\n",
    "for i in range(len(metric_list)):\n",
    "    metric = metric_list[i]\n",
    "    temp_df = pd.DataFrame(convert_dict[metric])\n",
    "    temp_df = temp_df[temp_df['allele'].isin(consensus_rare_alleles)]\n",
    "    stat_list = ViolinPlotStat(temp_df, 'method', 'value', box_pairs,\n",
    "                               '{}/CommonUnobservedAllele{}.png'.format(output_dir, metric), ylabel=metric)\n",
    "    plt.show()\n",
    "    temp_df = pd.DataFrame(stat_list)\n",
    "    temp_df['metric'] = metric\n",
    "    stat_df = pd.concat([stat_df, temp_df])\n",
    "\n",
    "stat_df = PrintStatDF(stat_df)\n",
    "display(stat_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
