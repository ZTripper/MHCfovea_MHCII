{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logomaker as lm\n",
    "from collections import OrderedDict\n",
    "from util import *\n",
    "from tqdm.notebook import tqdm\n",
    "from venn import venn, generate_petal_labels, draw_venn\n",
    "from statannot import add_stat_annotation\n",
    "from scipy.stats.mstats import ttest_rel, ttest_ind\n",
    "from IPython.display import display\n",
    "\n",
    "data_dir = '' # MHCfovea's dataframe directory\n",
    "main_dir = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dissimilar peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildSubSeq(seqs, length):\n",
    "    new_seqs = list()\n",
    "    for seq in seqs:\n",
    "        seq_len = len(seq)\n",
    "        temp_seqs = [seq[i:i+length] for i in range(seq_len - length + 1)]\n",
    "        new_seqs += temp_seqs\n",
    "    return new_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "train_hit_df = pd.read_csv('{}/train_hit.csv'.format(data_dir), index_col=0)\n",
    "valid_df = pd.read_csv('{}/valid.csv'.format(data_dir), index_col=0)\n",
    "test_df = pd.read_csv('{}/benchmark.csv'.format(data_dir), index_col=0)\n",
    "\n",
    "# combine training and validation\n",
    "train_df = pd.concat([train_hit_df, valid_df])\n",
    "\n",
    "# get allele counts\n",
    "train_allele_counts = train_df['mhc'].value_counts()\n",
    "test_allele_counts = test_df['mhc'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## analyzing similar peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar peptides in training\n",
    "\n",
    "train_peptides = train_df[train_df['bind']==1]['sequence'].unique()\n",
    "test_peptides = test_df[test_df['bind']==1]['sequence'].unique()\n",
    "\n",
    "train_peptide_length_dict = dict()\n",
    "for peptide in train_peptides:\n",
    "    length = len(peptide)\n",
    "    if train_peptide_length_dict.get(length):\n",
    "        train_peptide_length_dict[length].append(peptide)\n",
    "    else:\n",
    "        train_peptide_length_dict[length] = [peptide,]\n",
    "\n",
    "leave_train_peptides = list()\n",
    "for length, peptides in train_peptide_length_dict.items():\n",
    "    comp_peptides = BuildSubSeq(test_peptides, length)\n",
    "    left_peptides = list(set(peptides) - set(comp_peptides))\n",
    "    leave_train_peptides += left_peptides\n",
    "    \n",
    "outfile = '{}/dissimilar_train_peptides.json'.format(main_dir)\n",
    "json.dump(leave_train_peptides, open(outfile, 'w'))\n",
    "    \n",
    "print(\"Peptide Number of Training Dataset\")\n",
    "print(\"Origin training: \", train_df.shape[0])\n",
    "print(\"After removing duplicates: \", len(train_peptides))\n",
    "print(\"After removing similar peptide set: \", len(leave_train_peptides))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar peptides in benchmark\n",
    "\n",
    "train_peptides = train_df[train_df['bind']==1]['sequence'].unique()\n",
    "test_peptides = test_df[test_df['bind']==1]['sequence'].unique()\n",
    "\n",
    "test_peptide_length_dict = dict()\n",
    "for peptide in test_peptides:\n",
    "    length = len(peptide)\n",
    "    if test_peptide_length_dict.get(length):\n",
    "        test_peptide_length_dict[length].append(peptide)\n",
    "    else:\n",
    "        test_peptide_length_dict[length] = [peptide,]\n",
    "\n",
    "leave_test_peptides = list()\n",
    "for length, peptides in test_peptide_length_dict.items():\n",
    "    comp_peptides = BuildSubSeq(train_peptides, length)\n",
    "    left_peptides = list(set(peptides) - set(comp_peptides))\n",
    "    leave_test_peptides += left_peptides\n",
    "    \n",
    "outfile = '{}/dissimilar_test_peptides.json'.format(main_dir)\n",
    "json.dump(leave_test_peptides, open(outfile, 'w'))\n",
    "\n",
    "print(\"Peptide Number of Testing Dataset\")\n",
    "print(\"Origin testing: \", test_df[test_df['bind']==1].shape[0])\n",
    "print(\"After removing duplicates: \", len(test_peptides))\n",
    "print(\"After removing similar peptide set: \", len(leave_test_peptides))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar decoys in benchmark\n",
    "\n",
    "# train_decoy\n",
    "train_decoy_df_list = list()\n",
    "for i in range(90):\n",
    "    train_decoy_df_list.append(pd.read_csv(\"{}/train_decoy_{}.csv\".format(data_dir, i+1), index_col=0))\n",
    "\n",
    "train_decoy_df_list.append(train_df[train_df['bind']==0])\n",
    "train_decoy_df = pd.concat(train_decoy_df_list)\n",
    "\n",
    "train_decoys = train_decoy_df['sequence'].unique()\n",
    "test_decoys = test_df[test_df['bind']==0]['sequence'].unique()\n",
    "\n",
    "test_decoy_length_dict = dict()\n",
    "for decoy in test_decoys:\n",
    "    length = len(decoy)\n",
    "    if test_decoy_length_dict.get(length):\n",
    "        test_decoy_length_dict[length].append(decoy)\n",
    "    else:\n",
    "        test_decoy_length_dict[length] = [decoy,]\n",
    "\n",
    "leave_test_decoys = list()\n",
    "for length, decoys in test_decoy_length_dict.items():\n",
    "    comp_decoys = BuildSubSeq(train_decoys, length)\n",
    "    left_decoys = list(set(decoys) - set(comp_decoys))\n",
    "    leave_test_decoys += left_decoys\n",
    "    \n",
    "outfile = '{}/dissimilar_test_decoys.json'.format(main_dir)\n",
    "json.dump(leave_test_decoys, open(outfile, 'w'))\n",
    "\n",
    "print(\"Decoy Number of Testing Dataset\")\n",
    "print(\"Origin testing: \", test_df[test_df['bind']==0].shape[0])\n",
    "print(\"After removing duplicates: \", len(test_decoys))\n",
    "print(\"After removing similar decoy set: \", len(leave_test_decoys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## separating benchmark to similar and dissimilar peptides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add peptide tags\n",
    "\n",
    "save_file = '{}/benchmark_prediction.csv'.format(main_dir)\n",
    "\n",
    "leave_test_peptides = json.load(open('{}/dissimilar_test_peptides.json'.format(main_dir)))\n",
    "leave_test_decoys = json.load(open('{}/dissimilar_test_decoys.json'.format(main_dir)))\n",
    "\n",
    "test_df['peptide_tag'] = 'similar'\n",
    "test_df.loc[(test_df['bind']==1) & (test_df['sequence'].isin(leave_test_peptides)), 'peptide_tag'] = 'dissimilar'\n",
    "test_df.loc[(test_df['bind']==0) & (test_df['sequence'].isin(leave_test_decoys)), 'peptide_tag'] = 'dissimilar'\n",
    "'''\n",
    "# common unobserved alleles\n",
    "unobserved_alleles = ['C*03:02', 'A*24:07', 'A*36:01', 'B*38:02', 'C*04:03', 'A*34:02',\n",
    "                      'C*14:03', 'B*35:07', 'B*07:04', 'A*34:01', 'B*40:06']\n",
    "'''\n",
    "unobserved_alleles = ['A*24:07', 'A*33:03', 'A*34:01', 'A*34:02', 'A*36:01', 'B*07:04', 'B*15:10', 'B*35:07',\n",
    "                      'B*38:02', 'B*40:06', 'B*55:01', 'B*55:02', 'C*03:02', 'C*04:03', 'C*08:01', 'C*14:03']\n",
    "\n",
    "test_df['allele_tag'] = 'observed'\n",
    "test_df.loc[test_df['mhc'].isin(unobserved_alleles), 'allele_tag'] = 'unobserved'\n",
    "\n",
    "test_df.to_csv(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "def FixPosNegRatio(df, ratio):\n",
    "    pos_num = df[df['bind']==1].shape[0]\n",
    "    neg_num = df[df['bind']==0].shape[0]\n",
    "    if neg_num > pos_num*ratio:\n",
    "        select_pos_num = pos_num\n",
    "        select_neg_num = int(pos_num*ratio)\n",
    "    else:\n",
    "        select_pos_num = int(neg_num / ratio)\n",
    "        select_neg_num = neg_num\n",
    "    \n",
    "    select_pos_df = df[df['bind']==1].sample(n=select_pos_num, random_state=0)\n",
    "    select_neg_df = df[df['bind']==0].sample(n=select_neg_num, random_state=0)\n",
    "    \n",
    "    return pd.concat([select_pos_df, select_neg_df], ignore_index=True)\n",
    "\n",
    "\n",
    "test_pred_file = '{}/benchmark_prediction.csv'.format(main_dir)\n",
    "test_pred_df = pd.read_csv(test_pred_file, index_col=0)\n",
    "\n",
    "display(test_pred_df.groupby(['bind', 'peptide_tag', 'allele_tag']).count())\n",
    "\n",
    "# predictors\n",
    "tools = ['NetMHCpan4.1', 'MHCflurry2.0', 'MixMHCpred2.1', 'MHCfovea']\n",
    "test_pred_df = test_pred_df[~test_pred_df['MixMHCpred2.1'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performance\n",
    "\n",
    "ratio = test_pred_df[test_pred_df['bind']==0].shape[0] / test_pred_df[test_pred_df['bind']==1].shape[0]\n",
    "\n",
    "perform_dict_list = list()\n",
    "\n",
    "for peptide_tag in ['all', 'similar', 'dissimilar']:\n",
    "    for allele_tag in ['all', 'observed', 'unobserved']:\n",
    "        # peptide tag\n",
    "        if peptide_tag == 'all':\n",
    "            temp_df = test_pred_df\n",
    "        else:\n",
    "            temp_df = test_pred_df[test_pred_df['peptide_tag']==peptide_tag]\n",
    "        \n",
    "        # allele tag\n",
    "        if allele_tag == 'all':\n",
    "            temp_df = temp_df\n",
    "        else:\n",
    "            temp_df = temp_df[temp_df['allele_tag']==allele_tag]\n",
    "            \n",
    "        # fix ratio\n",
    "        if not ((peptide_tag == 'all') and (allele_tag == 'all')):\n",
    "            temp_df = FixPosNegRatio(temp_df, ratio)\n",
    "        \n",
    "        # performance\n",
    "        temp_y = temp_df['bind'].to_numpy()\n",
    "        for tool in tools:\n",
    "            temp_metrics = CalculateMetrics(temp_y, temp_df[tool].to_numpy())\n",
    "        \n",
    "            # add to dict list\n",
    "            pos_num = temp_df[temp_df['bind']==1].shape[0]\n",
    "            neg_num = temp_df[temp_df['bind']==0].shape[0]\n",
    "            perform_dict_list.append({\n",
    "                'positive_num': pos_num,\n",
    "                'negative_num': neg_num,\n",
    "                'ratio': neg_num / pos_num,\n",
    "                'peptide_tag': peptide_tag,\n",
    "                'allele_tag': allele_tag,\n",
    "                'predictor': tool,\n",
    "                'AUC': temp_metrics['AUC'],\n",
    "                'AUC0.1': temp_metrics['AUC0.1'],\n",
    "                'AP': temp_metrics['AP'],\n",
    "                'PPV': temp_metrics['PPV'],\n",
    "            })\n",
    "        \n",
    "perform_df = pd.DataFrame(perform_dict_list)\n",
    "perform_df.to_csv('{}/test_perform_by_groups.csv'.format(main_dir))\n",
    "display(perform_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load perform df\n",
    "\n",
    "perform_df = pd.read_csv('{}/test_perform_by_groups.csv'.format(main_dir), index_col=0)\n",
    "display(perform_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# distribution of peptides with different tags in benchmark\n",
    "\n",
    "test_pred_df['Peptide Tag'] = np.nan\n",
    "\n",
    "test_pred_df.loc[(test_pred_df['bind']==1) & (test_pred_df['peptide_tag']=='similar'), 'Peptide Tag'] = 'Positive similar'\n",
    "test_pred_df.loc[(test_pred_df['bind']==1) & (test_pred_df['peptide_tag']=='dissimilar'), 'Peptide Tag'] = 'Positive dissimilar'\n",
    "test_pred_df.loc[(test_pred_df['bind']==0) & (test_pred_df['peptide_tag']=='similar'), 'Peptide Tag'] = 'Negative similar'\n",
    "test_pred_df.loc[(test_pred_df['bind']==0) & (test_pred_df['peptide_tag']=='dissimilar'), 'Peptide Tag'] = 'Negative dissimilar'\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 3), dpi=600)\n",
    "sns.histplot(data=test_pred_df, hue='Peptide Tag', x='MHCfovea', ax=ax,\n",
    "             stat='probability', binwidth=0.02, common_norm=False, element='step')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('{}/benchmark_peptide_dist.png'.format(main_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for comparison of performance\n",
    "\n",
    "## add tags\n",
    "perform_df['tag'] = np.nan\n",
    "\n",
    "for p in ['similar', 'dissimilar']:\n",
    "    for a in ['observed', 'unobserved']:\n",
    "        perform_df.loc[(perform_df['peptide_tag'] == p) & (perform_df['allele_tag'] == a), 'tag'] = '{} alleles - {} peptides'.format(a, p)\n",
    "\n",
    "temp_df = perform_df[~perform_df['tag'].isna()]\n",
    "temp_df = temp_df.sort_values(by='tag', ascending=False)\n",
    "\n",
    "## plot by metrics\n",
    "metric_to_value = {'AUC': 0.9, 'AP':0.7}\n",
    "for metric in metric_to_value.keys():\n",
    "    value = metric_to_value[metric]\n",
    "    temp_df[metric] = temp_df[metric] - value\n",
    "    \n",
    "    fig, ax = plt.subplots(1,1, figsize=(8, 4), dpi=600)\n",
    "    order = ['MHCfovea', 'NetMHCpan4.1', 'MHCflurry2.0', 'MixMHCpred2.1']\n",
    "    sns.barplot(data=temp_df, x='predictor', hue='tag', y=metric, ax=ax, order=order, palette='muted')\n",
    "    \n",
    "    ax.set_yticks([i*(1-value)/10 for i in range(10)])\n",
    "    ax.set_yticklabels([str(np.round(i*(1-value)/10 + value, 2)) for i in range(10)])\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.legend(bbox_to_anchor=(0, 1), loc='lower left', ncol=2)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig('{}/test_{}_by_group.png'.format(main_dir, metric.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "train_hit_file = '{}/train_hit.csv'.format(data_dir)\n",
    "test_file = '{}/test.csv'.format(data_dir)\n",
    "\n",
    "train_df = pd.read_csv(train_hit_file, index_col=0)\n",
    "test_df = pd.read_csv(test_file, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build negatives from training positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive sequences\n",
    "neg_df = train_df[train_df['bind']==1]\n",
    "neg_df = neg_df.drop_duplicates(subset='sequence')\n",
    "\n",
    "# pytorch dataset\n",
    "neg_dataset = BuildDataset(neg_df, 'onehot', 15, with_label=True)\n",
    "torch.save(neg_dataset, '{}/neg/neg.pt'.format(main_dir))\n",
    "\n",
    "neg_df.to_csv('{}/neg/neg.csv'.format(main_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shell script for prediction\n",
    "\n",
    "alleles = list(sorted(test_df['mhc'].unique()))\n",
    "split_num = 8\n",
    "\n",
    "for i in range(0, len(alleles), split_num):\n",
    "    shell_str = list()\n",
    "    shell_str.append(\"#! /bin/bash\\n\")\n",
    "    shell_str.append(\"python3 predictor.py\")\n",
    "    shell_str.append(\" --mhc_file ../data/MHCI_res182_seq.json\")\n",
    "    shell_str.append(\" --rank_file ../data/score_rank.csv\")\n",
    "    shell_str.append(\" --peptide_dataframe {}/neg/neg.csv\".format(main_dir))\n",
    "    shell_str.append(\" --peptide_dataset {}/neg/neg.pt\".format(main_dir))\n",
    "    shell_str.append(\" --model_file model.py\")\n",
    "    shell_str.append(\" --model_state_dir ${TRAIN_RESUILT_DIR}/model_state\")\n",
    "    shell_str.append(\" --output_dir {}/neg/{}\".format(main_dir, i//split_num+1))\n",
    "    shell_str.append(\" --alleles '{}'\".format(','.join(alleles[i: i+split_num])))\n",
    "    \n",
    "    with open('{}/neg/run_pred_{}.sh'.format(main_dir, i//split_num+1), 'w') as f:\n",
    "        f.write(''.join(shell_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_legend(ax, new_loc, **kws):\n",
    "    old_legend = ax.legend_\n",
    "    handles = old_legend.legendHandles\n",
    "    labels = [t.get_text() for t in old_legend.get_texts()]\n",
    "    title = old_legend.get_title().get_text()\n",
    "    ax.legend(handles, labels, loc=new_loc, title=title, **kws)\n",
    "    \n",
    "    \n",
    "def get_motif_seqlogo(seqs, sub_motif_len=4):\n",
    "    aa_str = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "    seqs = seqs.apply(lambda x: x[:sub_motif_len] + x[-sub_motif_len:])\n",
    "    seqlogo_df = lm.alignment_to_matrix(sequences=seqs, to_type='information', characters_to_ignore='XU.')\n",
    "    df = pd.DataFrame(columns=list(aa_str))\n",
    "    df = pd.concat([df, seqlogo_df], axis=0)\n",
    "    df = df[list(aa_str)]\n",
    "    df = df.fillna(0.0)\n",
    "    return df\n",
    "\n",
    "    \n",
    "def motif_plot(seqlogo_df, side, ax, sub_motif_len=4, ylim=4, fontsize=10, title=None, turn_off_label=False):\n",
    "    if side == 'N':\n",
    "        xticklabels = list(range(1, sub_motif_len+1))\n",
    "    elif side == 'C':\n",
    "        xticklabels = list(range(-sub_motif_len, 0))\n",
    "    else: # both\n",
    "        xticklabels = list(range(1, sub_motif_len+1)) + list(range(-sub_motif_len, 0))\n",
    "    \n",
    "    logo = lm.Logo(seqlogo_df, color_scheme='skylign_protein', ax=ax)\n",
    "    \n",
    "    _ = ax.set_xticks(list(range(len(xticklabels))))\n",
    "    _ = ax.set_xticklabels(xticklabels)\n",
    "    _ = ax.set_ylim(0,ylim)\n",
    "    _ = ax.set_title(title)\n",
    "    \n",
    "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] + ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "        item.set_fontsize(fontsize)\n",
    "\n",
    "    if turn_off_label:\n",
    "        _ = ax.set_xticks([])\n",
    "        _ = ax.set_yticks([])\n",
    "        _ = ax.set_xticklabels([])\n",
    "        _ = ax.set_yticklabels([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_df = pd.read_csv('{}/benchmark_prediction.csv'.format(main_dir), index_col=0)\n",
    "\n",
    "pred_neg_df = pd.read_csv('{}/neg/neg.csv'.format(main_dir), index_col=0)\n",
    "\n",
    "for i in os.listdir('{}/neg/'.format(main_dir)):\n",
    "    if os.path.isdir('{}/neg/{}'.format(c, i)):\n",
    "        temp_df = pd.read_csv('{}/neg/{}/prediction.csv'.format(main_dir, i), index_col=0)\n",
    "        pred_neg_df = pred_neg_df.merge(temp_df, on='sequence', how='left')\n",
    "        \n",
    "display(preg_neg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the distribution of all peptides with different resouces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build df\n",
    "\n",
    "score_df = pd.DataFrame()\n",
    "for allele in pred_neg_df.columns:\n",
    "    if not re.match(r'[ABC]\\*[0-9]+\\:[0-9]+', allele):\n",
    "        continue\n",
    "    temp_df = pred_neg_df[['mhc', 'sequence', 'source', allele]]\n",
    "    temp_df = temp_df[temp_df['mhc'] != allele]\n",
    "    temp_df = temp_df.rename(columns={allele: 'score'})\n",
    "    temp_df['source'] = 'aritificial dataset'\n",
    "    score_df = pd.concat([score_df, temp_df], axis=0)\n",
    "    \n",
    "temp_df = pred_test_df[pred_test_df['bind']==1]\n",
    "temp_df['source'] = 'positives in the benchmark'\n",
    "temp_df = temp_df[['mhc', 'sequence', 'source', 'MHCfovea']]\n",
    "temp_df = temp_df.rename(columns={'MHCfovea': 'score'})\n",
    "score_df = pd.concat([score_df, temp_df], axis=0)\n",
    "\n",
    "temp_df = pred_test_df[pred_test_df['bind']==0]\n",
    "temp_df['source'] = 'negatives in the benchmark'\n",
    "temp_df = temp_df[['mhc', 'sequence', 'source', 'MHCfovea']]\n",
    "temp_df = temp_df.rename(columns={'MHCfovea': 'score'})\n",
    "score_df = pd.concat([score_df, temp_df], axis=0)\n",
    "\n",
    "display(score_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 3), dpi=600)\n",
    "sns.histplot(data=score_df, hue='source', x='score', ax=ax,\n",
    "             stat='probability', binwidth=0.02, common_norm=False, element='step')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('{}/benchmark_peptide_dist_with_neg.png'.format(main_dir))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
